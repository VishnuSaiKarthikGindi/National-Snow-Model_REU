{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582b19b-a266-4a82-9f9d-7d5ad1e3a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# Dataframe Packages\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Vector Packages\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# Raster Packages\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rioxarray.merge import merge_arrays\n",
    "import rasterstats as rs\n",
    "import gdal\n",
    "from gdal import gdalconst\n",
    "\n",
    "# Data Access Packages\n",
    "import earthaccess as ea\n",
    "import h5py\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from pystac_client import Client\n",
    "import richdem as rd\n",
    "import planetary_computer\n",
    "from planetary_computer import sign\n",
    "\n",
    "# General Packages\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import math\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import progress\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "from retrying import retry\n",
    "import fiona\n",
    "import re\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6edbc-c701-42b3-89c9-4649ac4dccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NSIDC_Data\n",
    "\n",
    "class ASODataTool:\n",
    "    def __init__(self, short_name, version, polygon='', filename_filter=''):\n",
    "        self.short_name = short_name\n",
    "        self.version = version\n",
    "        self.polygon = polygon\n",
    "        self.filename_filter = filename_filter\n",
    "        self.url_list = []\n",
    "        self.CMR_URL = 'https://cmr.earthdata.nasa.gov'\n",
    "        self.CMR_PAGE_SIZE = 2000\n",
    "        self.CMR_FILE_URL = ('{0}/search/granules.json?provider=NSIDC_ECS'\n",
    "                             '&sort_key[]=start_date&sort_key[]=producer_granule_id'\n",
    "                             '&scroll=true&page_size={1}'.format(self.CMR_URL, self.CMR_PAGE_SIZE))\n",
    "\n",
    "    def cmr_search(self, time_start, time_end, bounding_box):\n",
    "        try:\n",
    "            if not self.url_list:\n",
    "                self.url_list = NSIDC_Data.cmr_search(\n",
    "                    self.short_name, self.version, time_start, time_end,\n",
    "                    bounding_box=self.bounding_box, polygon=self.polygon,\n",
    "                    filename_filter=self.filename_filter, quiet=False)\n",
    "            return self.url_list\n",
    "        except KeyboardInterrupt:\n",
    "            quit()\n",
    "\n",
    "    def cmr_download(self, directory):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        NSIDC_Data.cmr_download(self.url_list, directory, False)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bounding_box(region):\n",
    "        regions = pd.read_pickle(\"C:\\\\Users\\\\VISH NU\\\\National_snow_model\\\\National-Snow-Model\\\\Data\\\\Processed\\\\RegionVal.pkl\")\n",
    "        superset = []\n",
    "\n",
    "        superset.append(regions[region])\n",
    "        superset = pd.concat(superset)\n",
    "        superset = gpd.GeoDataFrame(superset, geometry=gpd.points_from_xy(superset.Long, superset.Lat, crs=\"EPSG:4326\"))\n",
    "        bounding_box = list(superset.total_bounds)\n",
    "\n",
    "        return f\"{bounding_box[0]},{bounding_box[1]},{bounding_box[2]},{bounding_box[3]}\"\n",
    "\n",
    "class ASODownload(ASODataTool):\n",
    "    def __init__(self, short_name, version, polygon='', filename_filter=''):\n",
    "        super().__init__(short_name, version, polygon, filename_filter)\n",
    "        self.region_list =    [ 'N_Sierras',\n",
    "                                'S_Sierras',\n",
    "                                'Greater_Yellowstone',\n",
    "                                'N_Co_Rockies',\n",
    "                                'SW_Mont',\n",
    "                                'SW_Co_Rockies',\n",
    "                                'GBasin',\n",
    "                                'N_Wasatch',\n",
    "                                'N_Cascade',\n",
    "                                'S_Wasatch',\n",
    "                                'SW_Mtns',\n",
    "                                'E_WA_N_Id_W_Mont',\n",
    "                                'S_Wyoming',\n",
    "                                'SE_Co_Rockies',\n",
    "                                'Sawtooth',\n",
    "                                'Ca_Coast',\n",
    "                                'E_Or',\n",
    "                                'N_Yellowstone',\n",
    "                                'S_Cascade',\n",
    "                                'Wa_Coast',\n",
    "                                'Greater_Glacier',\n",
    "                                'Or_Coast'  ]\n",
    "\n",
    "    def select_region(self):\n",
    "        print(\"Select a region by entering its index:\")\n",
    "        for i, region in enumerate(self.region_list, start=1):\n",
    "            print(f\"{i}. {region}\")\n",
    "\n",
    "        try:\n",
    "            user_input = int(input(\"Enter the index of the region: \"))\n",
    "            if 1 <= user_input <= len(self.region_list):\n",
    "                selected_region = self.region_list[user_input - 1]\n",
    "                self.bounding_box = self.get_bounding_box(selected_region)\n",
    "                print(f\"You selected: {selected_region}\")\n",
    "                print(f\"Bounding Box: {self.bounding_box}\")\n",
    "            else:\n",
    "                print(\"Invalid index. Please select a valid index.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid index.\")\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    short_name = 'ASO_50M_SWE'\n",
    "    version = '1'\n",
    "\n",
    "    data_tool = ASODownload(short_name, version)\n",
    "    time_start = '2013-04-02T00:00:00Z'\n",
    "    time_end = '2019-07-19T23:59:59Z'\n",
    "    \n",
    "    selected_region = data_tool.select_region()  # Call select_region on the instance\n",
    "    directory = \"SWE_Data\"\n",
    "\n",
    "    print(f\"Fetching file URLs in progress for {selected_region} from {time_start} to {time_end}\")\n",
    "    url_list = data_tool.cmr_search(time_start, time_end, data_tool.bounding_box)\n",
    "    data_tool.cmr_download(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b8ea0-b7c7-4c08-8cf7-69398a1493f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASODataProcessing:\n",
    "    @staticmethod\n",
    "    def processing_tiff(input_file, output_res):\n",
    "        try:\n",
    "            date = os.path.splitext(input_file)[0].split(\"_\")[-1]\n",
    "            \n",
    "            output_folder = os.path.join(os.getcwd(), \"Processed_Data\")\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            output_file = os.path.join(output_folder, f\"ASO_100M_{date}.tif\")\n",
    "    \n",
    "            ds = gdal.Open(input_file)\n",
    "            if ds is None:\n",
    "                print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "                return None\n",
    "            \n",
    "            # Reproject and resample\n",
    "            gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "    \n",
    "            # Read the processed TIFF file using rasterio\n",
    "            rds = rxr.open_rasterio(output_file)\n",
    "            rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "            rds.name = \"data\"\n",
    "            df = rds.to_dataframe().reset_index()\n",
    "            return df\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_tiff_to_csv(output_res, config):\n",
    "\n",
    "        curr_dir = config[\"local_dir\"]\n",
    "        folder_path = os.path.join(curr_dir, \"SWE_Data\")\n",
    "        \n",
    "        if not os.path.exists(folder_path) or not os.path.isdir(folder_path):\n",
    "            print(f\"The folder '{input_folder}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        if not os.listdir(folder_path):\n",
    "            print(f\"The folder '{input_folder}' is empty.\")\n",
    "            return\n",
    "    \n",
    "        tiff_files = [filename for filename in os.listdir(folder_path) if filename.endswith(\".tif\")]\n",
    "    \n",
    "        for tiff_filename in tiff_files:\n",
    "            \n",
    "            tiff_filepath = os.path.join(folder_path, tiff_filename)\n",
    "            df = ASODataProcessing.processing_tiff(tiff_filepath, output_res)\n",
    "    \n",
    "            if df is not None:\n",
    "  \n",
    "                date = os.path.splitext(tiff_filename)[0].split(\"_\")[-1]\n",
    "    \n",
    "                # Define the CSV filename and folder\n",
    "                csv_filename = f\"ASO_SWE_{date}.csv\"\n",
    "                csv_folder = os.path.join(curr_dir, \"Processed_Data\", \"SWE_csv\")\n",
    "                os.makedirs(csv_folder, exist_ok=True)\n",
    "                csv_filepath = os.path.join(csv_folder, csv_filename)\n",
    "    \n",
    "                df.to_csv(csv_filepath, index=False)\n",
    "    \n",
    "                print(f\"Converted '{tiff_filename}' to '{csv_filename}'\")\n",
    "                \n",
    "\n",
    "    def process_folder(self, config):\n",
    "\n",
    "        pred_obs_metadata_df = check_and_fetch_meta(config)\n",
    "\n",
    "    \tdef create_polygon(self, row):\n",
    "        \treturn Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                        \t(row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                        \t(row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                        \t(row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "    \n",
    "        pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "        pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(create_polygon, axis=1)\n",
    "\n",
    "        metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "    \n",
    "        # Drop coordinates columns to reduce complexity for further processing\n",
    "        metadata_df = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                             'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                             'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                             'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "    \t\n",
    "    \tinput_folder = os.path.join(config[\"local_dir\"], \"Processed_Data\", \"swe_csv\")     #created in the previous function\n",
    "    \toutput_folder = os.path.join(config[\"local_dir\"], \"Processed_SWE\")                #create a new folder to store files that has fetched cell ids for lat/longs\n",
    "        csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    \n",
    "        for csv_file in csv_files:\n",
    "            input_aso_path = os.path.join(input_folder, csv_file)\n",
    "            output_aso_path = os.path.join(output_folder, csv_file)\n",
    "    \n",
    "            if os.path.exists(output_aso_path):\n",
    "                print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "                continue\n",
    "    \n",
    "            aso_swe_df = pd.read_csv(input_aso_path)\n",
    "            geometry = [Point(xy) for xy in zip(aso_swe_df['x'], aso_swe_df['y'])]\n",
    "\n",
    "            aso_swe_geo = gpd.GeoDataFrame(aso_swe_df, geometry=geometry)\n",
    "\n",
    "            result = gpd.sjoin(aso_swe_geo, metadata_df, how='left', predicate='within', op = 'intersects')\n",
    "    \n",
    "            Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "            Final_df.rename(columns={'data': 'swe'}, inplace=True)\n",
    "    \n",
    "            # Drop rows where 'cell_id' is NaN\n",
    "            if Final_df['cell_id'].isnull().values.any():\n",
    "                Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "    \n",
    "            Final_df.to_csv(output_aso_path, index=False)\n",
    "            print(f\"Processed {csv_file}\")\n",
    "        \n",
    "def check_and_fetch_meta(config):\n",
    "\n",
    "    file_path = config[\"grid_cells_meta\"],\n",
    "    s3_path = config[\"s3_url\"],\n",
    "    access_key = config[\"s3_access_key\"],\n",
    "    secret_key = config[\"s3_secret_key\"]\n",
    "\t\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File found locally\")\n",
    "\tdf = pd.read_csv(file_path)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"File not found locally, fetching from S3\")\n",
    "\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id = access_key,\n",
    "            aws_secret_access_key = secret_key,\n",
    "        )\n",
    "\n",
    "        s3 = session.client('s3')\n",
    "        local_file_name = os.path.basename(s3_path)\n",
    "        local_file_path = os.path.join(config[\"local_dir\"], local_file_name)\n",
    "        \n",
    "        try:\n",
    "            s3.get_object(Bucket = s3_path.split('/')[2], Key='/'.join(s3_path.split('/')[3:]))\n",
    "            csv_string = response['Body'].read().decode('utf-8')\n",
    "            df = pd.read_csv(StringIO(csv_string))\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading from S3: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Configuration file not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process data by processing TIFF and converting to CSV.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    config_path = os.cwd() + '/config.json'  #test\n",
    "    config = load_config(config_path)\n",
    "    if config is None:\n",
    "        logging.error(\"No configuration to proceed. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    data_processor = ASODataProcessing()\n",
    "    output_res = 0.001\n",
    "\n",
    "    data_processor.convert_tiff_to_csv(output_res, config)\n",
    "    data_processor.process_folder(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c91c7a-d327-4dbf-8669-1e667c0e4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aso_snotel_geometry(aso_swe_file, folder_path):\n",
    "    \n",
    "    aso_file = pd.read_csv(os.path.join(folder_path, aso_swe_file))\n",
    "    aso_file.set_index('cell_id', inplace=True)\n",
    "    aso_geometry = [Point(xy) for xy in zip(aso_file['x'], aso_file['y'])]\n",
    "    aso_gdf = gpd.GeoDataFrame(aso_file, geometry=aso_geometry)\n",
    "    \n",
    "    return aso_gdf\n",
    "\n",
    "def haversine_vectorized(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    lon1 = np.radians(lon1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    r = 6371.0\n",
    "    # Distance calculation\n",
    "    distances = r * c\n",
    "\n",
    "    return distances\n",
    "\n",
    "def calculate_nearest_snotel(aso_gdf, snotel_gdf, n=6, distance_cache=None):\n",
    "\n",
    "    if distance_cache is None:\n",
    "        distance_cache = {}\n",
    "\n",
    "    nearest_snotel = {}\n",
    "    for idx, aso_row in aso_gdf.iterrows():\n",
    "        cell_id = idx\n",
    "\n",
    "        if cell_id in distance_cache:\n",
    "            nearest_snotel[idx] = distance_cache[cell_id]\n",
    "        else:\n",
    "            distances = haversine_vectorized(\n",
    "                aso_row.geometry.y, aso_row.geometry.x,\n",
    "                snotel_gdf.geometry.y.values, snotel_gdf.geometry.x.values)\n",
    "\n",
    "            nearest_snotel[idx] = list(snotel_gdf['station_id'].iloc[distances.argsort()[:n]])\n",
    "            distance_cache[cell_id] = nearest_snotel[idx]\n",
    "\n",
    "    return nearest_snotel, distance_cache\n",
    "\n",
    "def calculate_distances_for_cell(aso_row, snotel_gdf, n=6):\n",
    "   \n",
    "    distances = haversine_vectorized(\n",
    "        aso_row.geometry.y, aso_row.geometry.x,\n",
    "        snotel_gdf.geometry.y.values, snotel_gdf.geometry.x.values)\n",
    "    \n",
    "    nearest_sites = list(snotel_gdf['station_id'].iloc[distances.argsort()[:n]])\n",
    "    \n",
    "    return nearest_sites\n",
    "\n",
    "def calculate_nearest_snotel_parallel(aso_gdf, snotel_gdf, n = 6, distance_cache = None):\n",
    "    \n",
    "    if distance_cache is None:\n",
    "        distance_cache = {}\n",
    "\n",
    "    nearest_snotel = {}\n",
    "    with ProcessPoolExecutor(max_workers = 16) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for idx, aso_row in aso_gdf.iterrows():\n",
    "            if idx not in distance_cache:\n",
    "                # Submit the task for parallel execution\n",
    "                futures.append(executor.submit(calculate_distances_for_cell, aso_row, snotel_gdf, n))\n",
    "            else:\n",
    "                nearest_snotel[idx] = distance_cache[idx]\n",
    "\n",
    "        # Retrieve results as they are completed\n",
    "        for future in tqdm(futures):\n",
    "            result = future.result()\n",
    "  \n",
    "            cell_id = result[0]  \n",
    "            nearest_snotel[cell_id] = result[1]\n",
    "            distance_cache[cell_id] = result[1]\n",
    "\n",
    "    return nearest_snotel, distance_cache\n",
    "\n",
    "def fetch_snotel_sites_for_cellids(config):\n",
    "    \"\"\"\n",
    "\tMight take longer to run depends on the size of the dataset. \n",
    "    \"\"\"\n",
    "    \n",
    "    metadata_df = check_and_fetch_meta(config)\n",
    "    #metadata_df['geometry'] = metadata_df['geometry'].apply(wkt.loads)\n",
    "    \n",
    "    def create_polygon(row):\n",
    "        return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                        (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                        (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                        (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "        \n",
    "    metadata_df = metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    metadata_df['geometry'] = metadata_df.apply(create_polygon, axis=1)\n",
    "    metadata = gpd.GeoDataFrame(metadata_df, geometry='geometry')\n",
    "\n",
    "    snotel_data = pd.read_csv(config['snotel_obs_path'])\n",
    "\n",
    "    date_columns = snotel_data.columns[1:]\n",
    "    new_column_names = {col: pd.to_datetime(col, format='%Y-%m-%d').strftime('%Y%m%d') for col in date_columns}\n",
    "    snotel_data_f = snotel_data.rename(columns=new_column_names)\n",
    "\n",
    "    snotel_file = pd.read_csv(config['snotel_ground_obs'])\n",
    "    snotel_geometry = [Point(xy) for xy in zip(snotel_file['longitude'], snotel_file['latitude'])]\n",
    "    snotel_gdf = gpd.GeoDataFrame(snotel_file, geometry=snotel_geometry)\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    folder_path = os.path.join(config['local_dir'], 'Processed_SWE')\n",
    "    for aso_swe_file in os.listdir(folder_path):\n",
    "\n",
    "        if os.path.isdir(os.path.join(folder_path, aso_swe_file)):\n",
    "            continue\n",
    "\n",
    "        timestamp = aso_swe_file.split('_')[-1].split('.')[0]\n",
    "        print(f\"Processing file with timestamp: {timestamp}\")\n",
    "\n",
    "        aso_gdf = load_aso_snotel_geometry(aso_swe_file, aso_swe_files_folder_path)\n",
    "        aso_swe_data = pd.read_csv(os.path.join(aso_swe_files_folder_path, aso_swe_file))\n",
    "\n",
    "        # Calculating nearest SNOTEL sites\n",
    "        nearest_snotel, distance_cache = calculate_nearest_snotel(aso_gdf, snotel_gdf, n=6)\n",
    "        print(f\"calculated nearest snotel for file with timestamp {timestamp}\")\n",
    "\n",
    "        transposed_data = {}\n",
    "\n",
    "        if timestamp in new_column_names.values():\n",
    "            for idx, aso_row in aso_gdf.iterrows():    \n",
    "                cell_id = idx\n",
    "                station_ids = nearest_snotel[cell_id]\n",
    "                selected_snotel_data = snotel_data_f[['station_id', timestamp]].loc[snotel_data_f['station_id'].isin(station_ids)]\n",
    "                station_mapping = {old_id: f\"nearest site {i+1}\" for i, old_id in enumerate(station_ids)}\n",
    "                \n",
    "                # Rename the station IDs in the selected SNOTEL data\n",
    "                selected_snotel_data['station_id'] = selected_snotel_data['station_id'].map(station_mapping)\n",
    "\n",
    "                # Transpose and set the index correctly\n",
    "                transposed_data[cell_id] = selected_snotel_data.set_index('station_id').T\n",
    "\n",
    "            transposed_df = pd.concat(transposed_data, axis=0)\n",
    "            \n",
    "            # Reset index and rename columns\n",
    "            transposed_df = transposed_df.reset_index()\n",
    "            transposed_df.rename(columns={'level_0': 'cell_id', 'level_1': 'Date'}, inplace = True)\n",
    "            transposed_df['Date'] = pd.to_datetime(transposed_df['Date'])\n",
    "        \n",
    "            aso_swe_data['Date'] = pd.to_datetime(timestamp)\n",
    "            aso_swe_data = aso_swe_data[['cell_id', 'Date', 'swe']]\n",
    "            merged_df = pd.merge(aso_swe_data, transposed_df, how='left', on=['cell_id', 'Date'])\n",
    "        \n",
    "            final_df = pd.concat([final_df, merged_df], ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "            aso_swe_data['Date'] = pd.to_datetime(timestamp)\n",
    "            aso_swe_data = aso_swe_data[['cell_id', 'Date', 'swe']]\n",
    "    \n",
    "            # No need to merge in this case, directly concatenate\n",
    "            final_df = pd.concat([final_df, aso_swe_data], ignore_index=True)\n",
    "\n",
    "    req_cols = ['cell_id', 'lat', 'lon', 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat', 'geometry']\n",
    "    Result = final_df.merge(metadata[req_cols], how='left', on='cell_id')\n",
    "\n",
    "    Result.rename(columns={'swe': 'ASO_SWE_in'}, inplace=True)\n",
    "    Result = Result[['cell_id', 'Date', 'ASO_SWE_in', 'lat', 'lon', 'nearest site 1', 'nearest site 2',\n",
    "                     'nearest site 3', 'nearest site 4', 'nearest site 5', 'nearest site 6',\n",
    "                     'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                     'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']]\n",
    "\n",
    "    output_filename = os.path.join(config['local_dir'], \"Provided_Data\", \"grid_cells_meta.csv\")\n",
    "    Result.to_csv(output_filename, index=False)\n",
    "    print(\"Processed and saved data\")\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process data by processing TIFF and converting to CSV.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    config_path = os.path.join(os.getcwd(), 'config.json')\n",
    "    config = load_config(config_path)\n",
    "    if config is None:\n",
    "        logging.error(\"No configuration to proceed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    fetch_snotel_sites_for_cellids(config)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c1329-412e-4047-a516-b6a2c4599c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_s3(input_file, s3_url, access_key=None, secret_key=None):\n",
    "\n",
    "    path = s3_url.split('/')\n",
    "    bucket_name = path[2]\n",
    "    object_name = '/'.join(path[3:]) \n",
    "\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=access_key,\n",
    "        aws_secret_access_key=secret_key,\n",
    "    )\n",
    "    \n",
    "    s3_client = session.client('s3')\n",
    "\n",
    "    try:\n",
    "        s3_client.upload_file(input_file, bucket_name, object_name)\n",
    "        print(f\"File {input_file} uploaded to {bucket_name}/{object_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {input_file} was not found\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"Error: Credentials not available\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "access_key = config['AWS_access_key']\n",
    "secret_key = config['AWS_secret_key']\n",
    "\n",
    "input_file = r\"/home/vgindi/Provided_Data/grid_cells_meta.csv\"\n",
    "s3_url = \"s3://national-snow-model/NSMv2.0/data/TrainingDFs/grid_cells_meta.csv\"\n",
    "upload_file_to_s3(input_file, s3_url, access_key, secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0f3a-7713-45d2-881f-574037b3a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Simple implementation of parallel processing using concurrency it takes so long to execute,\n",
    "Explore terrain_daskconcurrency and terrain-processing_cluster python for more optimized implementations.\n",
    "\"\"\"\n",
    "\n",
    "def process_single_location(args):\n",
    "    lat, lon, regions, tiles = args\n",
    "\n",
    "    if (lat, lon) in elevation_cache:\n",
    "        elev, slop, asp = elevation_cache[(lat, lon)]\n",
    "        return elev, slop, asp\n",
    "\n",
    "    tile_id = 'Copernicus_DSM_COG_30_N' + str(math.floor(lon)) + '_00_W' + str(math.ceil(abs(lat))) + '_00_DEM'\n",
    "    index_id = regions.loc[tile_id]['sliceID']\n",
    "\n",
    "    signed_asset = planetary_computer.sign(tiles[index_id].assets[\"data\"])\n",
    "\n",
    "    elevation = rxr.open_rasterio(signed_asset.href)\n",
    "    \n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "\n",
    "    no_data_value = -9999\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    temp_ds = driver.Create('', tilearray.shape[1], tilearray.shape[0], 1, gdalconst.GDT_Float32)\n",
    "\n",
    "    temp_ds.GetRasterBand(1).WriteArray(tilearray)\n",
    "    temp_ds.GetRasterBand(1).SetNoDataValue(no_data_value)\n",
    "    temp_ds.SetProjection('EPSG:4326')\n",
    "    temp_ds.SetGeoTransform(geo)\n",
    "\n",
    "    tilearray_np = temp_ds.GetRasterBand(1).ReadAsArray()\n",
    "    slope_arr, aspect_arr = np.gradient(tilearray_np)\n",
    "    aspect_arr = np.rad2deg(np.arctan2(aspect_arr[0], aspect_arr[1]))\n",
    "    \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "\n",
    "    elevation_cache[(lat, lon)] = (elev, slop, asp)  \n",
    "    return elev, slop, asp\n",
    "\n",
    "def extract_terrain_data_threaded(metadata_df, bounding_box, max_workers=10):\n",
    "    global elevation_cache \n",
    "\n",
    "    elevation_cache = {} \n",
    "    min_x, min_y, max_x, max_y = *bounding_box[0], *bounding_box[1]\n",
    "    \n",
    "    client = Client.open(\n",
    "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            ignore_conformance=True,\n",
    "        )\n",
    "\n",
    "    search = client.search(\n",
    "                    collections=[\"cop-dem-glo-90\"],\n",
    "                    intersects = {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": [[\n",
    "                            [min_x, min_y],\n",
    "                            [max_x, min_y],\n",
    "                            [max_x, max_y],\n",
    "                            [min_x, max_y],\n",
    "                            [min_x, min_y]  \n",
    "                        ]]})\n",
    "\n",
    "    tiles = list(search.items())\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    print(\"Retrieving Copernicus 90m DEM tiles\")\n",
    "    for i in tqdm(range(0, len(tiles))):\n",
    "        row = [i, tiles[i].id]\n",
    "        regions.append(row)\n",
    "    regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "    regions = regions.set_index(regions['tileID'])\n",
    "    del regions['tileID']\n",
    "\n",
    "    print(\"Interpolating Grid Cell Spatial Features\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_location, (metadata_df.iloc[i]['cen_lat'], metadata_df.iloc[i]['cen_lon'], regions, tiles))\n",
    "                   for i in tqdm(range(len(metadata_df)))]\n",
    "        \n",
    "        results = []\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    metadata_df['Elevation_m'], metadata_df['Slope_Deg'], metadata_df['Aspect_L'] = zip(*results)\n",
    "\n",
    "metadata_df = pd.read_csv(r\"/home/vgindi/Provided_Data/grid_cells_meta.csv\")\n",
    "metadata_df= metadata_df.head(20)\n",
    "bounding_box = ((-120.3763448720203, 36.29256774541929), (-118.292253412863, 38.994985247736324))    \n",
    "    \n",
    "extract_terrain_data_threaded(metadata_df, bounding_box)\n",
    "\n",
    "# Display the results\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b0f0-1c38-4ba3-8aed-1ca6b97c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This code block crops the global coverage VIIRS data to south sierras subregion. \n",
    "\n",
    "from rasterio.errors import WindowError \n",
    "\n",
    "def crop_sierras(input_file_path, output_file_path, shapes):\n",
    "    try:\n",
    "        with rasterio.open(input_file_path) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "            out_meta = src.meta\n",
    "            out_meta.update({\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": out_image.shape[1],\n",
    "                \"width\": out_image.shape[2],\n",
    "                \"transform\": out_transform\n",
    "            })\n",
    "            with rasterio.open(output_file_path, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "    except (WindowError, ValueError) as e:\n",
    "        print(f\"Error processing {input_file_path}: {e}\")\n",
    "\n",
    "def download_viirs_sca(config):\n",
    "    \n",
    "    # Load shapes from the shapefile\n",
    "    with fiona.open(config['low_sierras_shapefile'], 'r') as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "\n",
    "    input_dir = os.path.join(config['local_dir'], \"VIIRS_Data\")\n",
    "\n",
    "    for year_folder in os.listdir(input_dir):\n",
    "        year_folder_path = os.path.join(input_dir, year_folder)\n",
    "        \n",
    "        if os.path.isdir(year_folder_path):   \n",
    "            year = re.search(r'\\d{4}', year_folder).group()\n",
    "            output_year_folder = os.path.join(config['local_dir'], \"VIIRS_Sierras\", year)\n",
    "            os.makedirs(output_year_folder, exist_ok=True)\n",
    "        \n",
    "            for file_name in os.listdir(year_folder_path):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    if 'h08v05' in file_name:   \n",
    "                        parts = file_name.split('_')\n",
    "                        output_file_name = '_'.join(parts[:3]) + '.tif'\n",
    "                        output_file_path = os.path.join(output_year_folder, output_file_name)\n",
    "                        input_file_path = os.path.join(year_folder_path, file_name)\n",
    "                        if not os.path.exists(output_file_path):\n",
    "                            crop_sierras(input_file_path, output_file_path, shapes)\n",
    "                            print(f\"Processed and saved {output_file_path}\")\n",
    "                        else:\n",
    "                            print(f\"Skipping {output_file_name} as it already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab2744-b080-48c8-bb77-f4b9c14ca774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This code cell transforms the raw VIIRS tiff files to 100m resolution and saves each file in .csv format\n",
    "\n",
    "def processing_VIIRS(input_file, output_res, config):\n",
    "    try:\n",
    "        output_folder_tiff = os.path.join(config['local_dir'], \"Processed_VIIRS\", os.path.basename(os.path.dirname(input_file)))\n",
    "        os.makedirs(output_folder_tiff, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder_tiff, os.path.basename(input_file))\n",
    "\n",
    "        # Reproject and resample\n",
    "        ds = gdal.Open(input_file)\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "            return None\n",
    "        \n",
    "        gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "\n",
    "        # Read the processed TIFF file using rasterio\n",
    "        rds = rxr.open_rasterio(output_file)\n",
    "        rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "        rds.name = \"data\"\n",
    "        df = rds.to_dataframe().reset_index()\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_and_convert_viirs(output_res, config):\n",
    "\n",
    "    input_dir = os.path.join(config['loacl_dir'], \"VIIRS_Sierras\")\n",
    "    for year in os.listdir(input_dir):\n",
    "        year_dir = os.path.join(input_dir, year)\n",
    "        \n",
    "        if os.path.isdir(year_dir):\n",
    "            for file_name in os.listdir(year_dir):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    input_file_path = os.path.join(year_dir, file_name)\n",
    "                    df = processing_VIIRS(input_file_path, output_res, config)\n",
    "                    \n",
    "                    if df is not None:\n",
    "                        csv_folder = os.path.join(config['local_dir'], \"Processed_VIIRS\", \"VIIRS_csv\")\n",
    "                        os.makedirs(csv_folder, exist_ok=True)\n",
    "                        csv_file_path = os.path.join(csv_folder, file_name.replace('.tif', '.csv'))\n",
    " \n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        print(f\"Processed and saved {csv_file_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process data by processing TIFF and converting to CSV.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    config_path = os.path.join(os.getcwd(), 'config.json') \n",
    "    config = load_config(config_path)\n",
    "    if config is None:\n",
    "        logging.error(\"No configuration to proceed. Exiting.\")\n",
    "        return\n",
    "        \n",
    "\tdownload_viirs_sca(config)\n",
    "    output_res = 0.001 # approximate value in meters, actual value is 0.0009 \n",
    "    process_and_convert_viirs(output_res, config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2831c-817d-40b5-a2e0-d9ebff8a5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell fetches the cell id using grid_cells_meta_idx metadata for each lat/lon pair for VIIRS csv file\n",
    "\"\"\"\n",
    "def create_polygon(row):\n",
    "    return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                    (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                    (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                    (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "    \n",
    "def process_folder(config):\n",
    "\n",
    "    pred_obs_metadata_df = check_and_fetch_meta(config)\n",
    "\n",
    "    pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(create_polygon, axis=1)\n",
    "\n",
    "    metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "\n",
    "    metadata = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                    'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                    'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                    'UL_Coord_Long', 'UL_Coord_Lat', 'index_id', 'lat', 'lon'], axis=1)\n",
    "\t\n",
    "    input_folder = os.path.join(config['local_dir'], \"Processed_VIIRS\", \"VIIRS_csv\")\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\t\n",
    "    output_folder = os.path.join(config['local_dir'], \"VIIRS_cellids\")\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(input_folder, csv_file)          \n",
    "        output_path = os.path.join(output_folder, csv_file)\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        viirs_sca_df = pd.read_csv(input_path)\n",
    "\n",
    "        geometry = [Point(xy) for xy in zip(viirs_sca_df['x'], viirs_sca_df['y'])]\n",
    "        viirs_sca_geo = gpd.GeoDataFrame(viirs_sca_df, geometry=geometry)\n",
    "        result = gpd.sjoin(viirs_sca_geo, metadata, how='left', predicate='within', op = 'intersects')\n",
    "\n",
    "        # Select specific columns for the final DataFrame\n",
    "        Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "        Final_df.rename(columns={'data': 'VIIRS_SCA'}, inplace=True)\n",
    "\n",
    "        if Final_df['cell_id'].isnull().values.any():\n",
    "            Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "\n",
    "        Final_df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "        \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process data by processing TIFF and converting to CSV.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    config_path = os.path.join(os.getcwd(), 'config.json') \n",
    "    config = load_config(config_path)\n",
    "    if config is None:\n",
    "        logging.error(\"No configuration to proceed. Exiting.\")\n",
    "        return\n",
    "    process_folder(config)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NSM",
   "language": "python",
   "name": "nsm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
